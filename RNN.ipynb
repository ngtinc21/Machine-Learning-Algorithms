{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP4BjJUf1l+5HtGsniLfd22",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ngtinc21/Machine-Learning-Algorithms/blob/main/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Overview of Recurrent Neural Network**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "A recurrent neural network (RNN) is a special type of an artificial neural network - \n",
        "\n",
        "\n",
        "*   adapted to work for time series data or data that involves sequences, \n",
        "*   such as, natural language processing (NLP), daily stock prices, image capturing, or sensor measurements\n",
        "\n",
        "***Recurrent*** means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the **preceding elements**.\n",
        "\n",
        "The figure below shows a convertion from a Feed-Forward Neural Network into a RNN. The nodes in different layers of the neural network are **compressed to form a single layer** of recurrent neural networks. A, B, and C are the network parameters, used to improve the output of the model.\n",
        "\n",
        "“x” is the input layer, “h” is the hidden layer, and “y” is the output layer. \n",
        "\n",
        "At any given time t, the current input is a combination of input at x(t) and x(t-1). The output at any given time is fetched back to the network to improve on the output.\n",
        "\n",
        "![image.png](https://www.simplilearn.com/ice9/free_resources_article_thumb/Simple_Recurrent_Neural_Network.png)\n",
        "\n",
        "![image.png](https://www.simplilearn.com/ice9/free_resources_article_thumb/Fully_connected_Recurrent_Neural_Network.gif)"
      ],
      "metadata": {
        "id": "B-iw-YE67ose"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Why Uses RNN over the Feed-Forward Neural Nets (Conventional ANN)?**\n",
        "\n",
        "---\n",
        "\n",
        "## **Problems of Conventional ANN**\n",
        "1.   Cannot handle sequential data\n",
        "2.   Considers only the current input\n",
        "3.   Cannot memorize previous inputs\n",
        "\n",
        "> Solutions of these issues: ***RNN***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "04FJZiBNYev_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working Mechanisms of RNN**\n",
        "\n",
        "---\n",
        "Below figure shows the representation of a **foward propagating RNN** with mathematical details.  \n",
        "\n",
        "1.   The input layer takes in the input \\*\\**$x_t$*** to the neural network and processes it and passes it onto the middle layer \\**\\**$a_t$**. \n",
        "\n",
        "2.   The middle layer can consist of multiple hidden layers, each with its own activation functions ($g_a$ and $g_y$) and weights ($W_{aa}$, $W_{ax}$ and $W_{ya}$) and biases ($b_a$ and $b_y$)\n",
        "\n",
        "3.   The RNN will **standardize** the different activation functions and weight and biases so that each hidden layer has the **same parameters**\n",
        "\n",
        "4.    Instead of creating hidden layers with various parameters, it will create one and loop over it as many times as required.\n",
        "\n",
        "## **Mathematical representation**\n",
        "> The involved parameters of the RNN as shown in the figure are as followed:\n",
        "$\\require{color}$\n",
        ">   - \\*\\*$a_t$** is the value of the hidden units/states at time $t$\n",
        ">   - \\*\\*$\\colorbox{orange}{\\(x_t\\)}$** is the input at time step $t$\n",
        ">   - \\*\\*$\\colorbox{yellowgreen}{\\(y_t\\)}$** is the output of the network at time step $t$\n",
        ">   - \\*\\*$\\color{red}{\\text{$W_{aa}$}}$** are weights associated with hidden units in recurrent layer\n",
        ">   - \\*\\*$\\color{purple}{\\text{$W_{ax}$}}$** are weights associated with inputs in recurrent layer\n",
        ">   - \\*\\*$\\color{cyan}{\\text{$W_{ya}$}}$** are weights associated with hidden to output layer\n",
        ">   - \\*\\*$b_a$** is the biase associated with the recurrent layer\n",
        ">   - \\*\\*$b_y$** is the biase associated with the output layer\n",
        ">   - \\*\\*$g_a$** is the activation function in the hidden layer used to compute activation value $a_t$\n",
        ">   - \\*\\*$g_y$** is the activation function in the output layer used to compute output value $y_t$\n",
        ">\n",
        "> ![image.png](https://camo.githubusercontent.com/694a18c571b70f2f15bd5d54f7caff19290c75f94a5e9cba5aaeba64e00ce57b/68747470733a2f2f666972656261736573746f726167652e676f6f676c65617069732e636f6d2f76302f622f646565702d6c6561726e696e672d63726173682d636f757273652e61707073706f742e636f6d2f6f2f36524e4e362e706e673f616c743d6d6564696126746f6b656e3d66663939333764362d613830392d346438312d396137662d386135653131366563643530)\n",
        ">\n",
        "> ### At time step $t$, the above RNN uses input value $x_t$ and activation value $a_{t-1}$ to predict output value $y_t$\n",
        "\n",
        "\n",
        "## **Further Simplification of the Above Notation**\n",
        ">  Combine \\*\\**$\\color{red}{\\text{$W_{aa}$}}$*** and \\**\\**$\\color{purple}{\\text{$W_{ax}$}}$** together to form a new parameter \\**\\**$\\color{green}{\\text{$W_a$}}$**\n",
        "> - Compressing the two parameter matrices into one can simplify the notation, especially for more complex model\n",
        "> - Similarly, \\*\\*$\\color{cyan}{\\text{$W_{ya}$}}$** is also changed to a new parameter \\**\\**$\\color{royalblue}{\\text{$W_y$}}$**\n",
        ">\n",
        "> ![image.png](https://camo.githubusercontent.com/4f55d6af69c8eb429fa3f2c749a579ee43cc189c139ed8950b10c286956773c1/68747470733a2f2f666972656261736573746f726167652e676f6f676c65617069732e636f6d2f76302f622f646565702d6c6561726e696e672d63726173682d636f757273652e61707073706f742e636f6d2f6f2f36524e4e372e706e673f616c743d6d6564696126746f6b656e3d61646531633232362d366436632d346330382d383361352d343265306364613335323737)\n",
        "\n",
        "## **Back Propagation Through Time (BPTT)**\n",
        "\n",
        "The backpropagation is the training algorithm used to **update the weights (or the gradients) of parameters** in a neural network in order to minimize the error between the expected output and the predicted output for a given input. For a RNN, the traditional backpropagation algorithm is specifically modified to include the recurrent loop in time to train the weights of the network. \n",
        "\n",
        "BPTT differs from the traditional approach in that BPTT **sums errors at each time step** whereas feedforward networks do not need to sum errors as they do not share parameters across each layer.\n",
        "\n",
        "**Since Keras is being used, there is no worry about how this happens behind the scenes**, what to concern is only about setting up the network correctly (at least at this stage)."
      ],
      "metadata": {
        "id": "OB3bsDt8Wru3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advantages and Disadvantages of RNNs**\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages:**\n",
        "\n",
        "1.   Ability to handle sequence data.\n",
        "2.   Ability to handle inputs of varying lengths.\n",
        "3.   Ability to store or ‘memorize’ historical information.\n",
        "\n",
        "### **Disadvantages:**\n",
        "\n",
        "1.   Possibly (very) slow computation\n",
        "\n",
        "2.   Network not taking into account future inputs to make decisions, as only previous information is use for prediction\n",
        "\n",
        "3.  Vanishing gradient problem\n",
        "  - the gradients used to compute the weight update may get very close to zero preventing the network from learning new weights\n",
        "  - the deeper the network, the more pronounced is this problem\n",
        "  - good for storing memory 3-4 instances of past iterations but larger number of instances don't provide good results\n",
        "\n",
        "4.  Exploding gradient problem\n",
        "  - accumulation of large error gradients can result in very large updates to the neural network model weights during the training proces\n"
      ],
      "metadata": {
        "id": "QqBQv7cgQkFt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Diffferent Architectures of CNN**\n",
        "\n",
        "---\n",
        "\n",
        "### **Types of RNN**\n",
        "\n",
        "*   One To One - Traditional neural networks\n",
        "*   One to Many - A single input can produce multiple outputs, e.g. music generation or text generation\n",
        "*   Many to One - Many inputs from different time steps produce a single outpu, e.g. sentiment analysis, emotion detection and text classification\n",
        "*   Many to Many - e.g. language translation system and voice recognition\n",
        "\n",
        "![image.png](https://miro.medium.com/max/1400/1*RsRIEyJyfgvisdW363CbDw.jpeg)\n",
        "<p align='right'>Image Source: Andrej Karpathy</p>"
      ],
      "metadata": {
        "id": "GBBEikhrPz8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **RNN Architectures**\n",
        "\n",
        "1.   Vanilla RNN\n",
        "  - Basic RNN architecture with self-looping as **described much by above**\n",
        "  - A hidden state to represent past knowledge, which is also served as an input to the current step and next step\n",
        "  - Every step of RNN shares the same activation function and the same sets of parameters\n",
        "  - BUT a brain with a single set of memory (only few set of parameters to process) is easily overloaded and cannot remember the past with some distance\n",
        "\n",
        "2.  Bidirectional recurrent neural networks (BRNN)\n",
        "  - **Inputs from future time steps are used** to improve the accuracy of the network\n",
        "  - Like having knowledge of the first and last words of a sentence to predict the middle words\n",
        "\n",
        "3.  Gated Recurrent Units (GRU)\n",
        "  - Designed to **handle the vanishing an exploding gradient problem**\n",
        "  - There are **reset and update gate**, in which they determine which information is to be retained for future predictions\n",
        "\n",
        "4.  Long Short Term Memory (LSTM)\n",
        "  - Designed to **address the vanishing gradient problem**\n",
        "  - There are three gates called **input, output and forget gate**. Similar to GRU, these gates determine which information to emphasize, to output and to forget, for the prediction\n",
        "  - Having three logical units of memories to process different dimensions greatly improves the capability of LSTM to remember both the long and short term information\n",
        "  - Instead of having a single neural network (recursive tanh hidden) layer, four interacting layers are communicating extraordinarily \n",
        "\n",
        "### **Long Short Term Meomry (LSTM)**\n",
        "Detailed Mechanism:\n",
        "\n",
        "![image.png](https://miro.medium.com/max/1400/1*cU0pLdq-KvhebSNYiF-hYw.png)\n",
        "\n",
        "LSTMs work in 3 steps:\n",
        "\n",
        "  1. Decide how much historical data it should remember by a **forget gate**\n",
        "      - determined by a sigmoid function\n",
        "      - the leftermost sigmoid function in the above plot\n",
        "\n",
        "      ![image.png](https://www.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step1.png)\n",
        "\n",
        "  2. Decide how much this unit adds to the current state by an **input gate**\n",
        "      - determined by a sigmoid function and a tanh function\n",
        "      - sigmoid function decides which values to let through (0 or 1), while tanh function gives weightage to the values which are passed, deciding their level of importance (-1 to 1)\n",
        "\n",
        "      ![image.png](https://www.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step2.png)\n",
        "\n",
        "  3. Decide what part of the current cell state makes it to the output by an **output gate**\n",
        "      - determined by a sigmoid function, which decides what parts of the cell state make it to the output\n",
        "      - Then, put the cell state through tanh to push the values to be between -1 and 1 and multiply it by the output of the sigmoid gate\n",
        "\n",
        "      ![image.png](https://s3.amazonaws.com/static2.simplilearn.com/ice9/free_resources_article_thumb/LSTMs_step3.png)\n",
        "\n",
        "> Graphical difference with RNN:\n",
        "![image.png](https://camo.githubusercontent.com/5e4e339bfad09c8532386f8befec19e2609fe5a77f4f302782da7472cf23b67c/68747470733a2f2f666972656261736573746f726167652e676f6f676c65617069732e636f6d2f76302f622f646565702d6c6561726e696e672d63726173682d636f757273652e61707073706f742e636f6d2f6f2f36524e4e32362e706e673f616c743d6d6564696126746f6b656e3d61623938643732302d633066392d346333322d613663342d633539376438646139303462)"
      ],
      "metadata": {
        "id": "p9Av2FbJfTxk"
      }
    }
  ]
}